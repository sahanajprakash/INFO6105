{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0c0e19",
   "metadata": {},
   "source": [
    "## Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff5f1d-90f5-46f3-a439-873c68ac9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fredapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc3efd-af91-494a-9501-3e7a604220eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a613da-01bd-45a6-93ec-b3d0fc078dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79123942",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library and tools\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas_datareader import DataReader\n",
    "from datetime import datetime\n",
    "import scipy.stats as ss\n",
    "import yfinance as yf\n",
    "\n",
    "start_date = datetime(2020,1,31)\n",
    "end_date = datetime(2024,8,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212196d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#download the datasets\n",
    "stock = yf.download('TSLA',start= start_date, end = end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb52298",
   "metadata": {},
   "source": [
    "## Stock Analysis and Competitor Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b53227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock Analysis and Competitor Summary(Assignment1)\n",
    "stock.info()\n",
    "print(stock.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the stock price history(Open, Close, High, Low)\n",
    "# Ensure index is datetime for Matplotlib compatibility\n",
    "stock.index = pd.to_datetime(stock.index)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot Open, Close, High, Low\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stock.index, stock['Open'], label='Open', color='blue', linewidth=1)\n",
    "plt.plot(stock.index, stock['Close'], label='Close', color='green', linewidth=1)\n",
    "plt.plot(stock.index, stock['High'], label='High', color='orange', linewidth=1)\n",
    "plt.plot(stock.index, stock['Low'], label='Low', color='red', linewidth=1)\n",
    "plt.title('Tesla Stock Price History (Open, Close, High, Low)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24b575",
   "metadata": {},
   "source": [
    "## Find 3 competitors and visualize their stock price history in the same time period - BMW, NIO, and BYD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['BMW.DE', 'NIO', 'BYD', ]\n",
    "\n",
    "#download the stock data from yfinance:\n",
    "data = yf.download(tickers, start = start_date, end = end_date )\n",
    "print(data.head())\n",
    "\n",
    "# Visualize Open, Close, High, Low prices for all tickers\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(2, 1, 1)# Loop through each ticker and plot Open, Close, High, and Low prices\n",
    "for ticker in tickers:\n",
    "    plt.plot(data['Open'][ticker], label=f'{ticker} Open Price', linestyle='--') \n",
    "    plt.plot(data['Close'][ticker], label=f'{ticker} Close Price')  \n",
    "    plt.plot(data['High'][ticker], label=f'{ticker} High Price', linestyle='-.')  \n",
    "    plt.plot(data['Low'][ticker], label=f'{ticker} Low Price', linestyle=':')  \n",
    "    \n",
    "plt.title('Stock Price Comparison (Jan 2020 - Aug 2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend() \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualize Volume traded for all tickers\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "\n",
    "# Loop through each ticker and plot trading volume\n",
    "for ticker in tickers:\n",
    "    plt.plot(data['Volume'][ticker], label=f'{ticker} Volume')\n",
    "\n",
    "# Set the title and axis labels for the volume plot\n",
    "plt.title('Volume Traded Comparison (Jan 2020 - Aug 2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d41cb3",
   "metadata": {},
   "source": [
    "## Extracting the adjusted close of Tesla, calculating mean, variance, skewness and kurtosis, and kernel density estimation¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the adjusted close column from the datasets\n",
    "adjusted_close = stock['Adj Close'].dropna()\n",
    "\n",
    "# calculate the mean, variance, skewness, and kurtosis of the adjusted close column\n",
    "mean_adj_close = np.mean(adjusted_close)\n",
    "variance_adj_close = np.var(adjusted_close)\n",
    "skewness_adj_close = ss.skew(adjusted_close)\n",
    "kurtosis_adj_close = ss.kurtosis(adjusted_close)\n",
    "\n",
    "#Display the values:\n",
    "print(\"mean:\", mean_adj_close)\n",
    "print(\"variance:\",variance_adj_close)\n",
    "print(\"Skewness:\", skewness_adj_close)\n",
    "print(\"Kurtosis:\", kurtosis_adj_close)\n",
    "sns.kdeplot(data=adjusted_close,linewidth=3)\n",
    "plt.title('Kernel Density Estimation of Tesla Adjusted Close Price')\n",
    "plt.xlabel('Adjusted Close Price (USD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb30fd9-d737-48e8-a7dd-f35b60b84649",
   "metadata": {},
   "source": [
    "## Feature Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be38a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "603a9a7e-3849-4cbf-a6ca-c56f9f567219",
   "metadata": {},
   "source": [
    "#### 3. Feature Selection from FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e755f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fred = Fred(api_key='add your api key and dont forget to remove it before commit')\n",
    "varList = [\n",
    "    'T10Y3M', 'DGS10', 'OBMMIJUMBO30YF',  # Term premium, 30-year jumbo mortgage\n",
    "    'DEXUSEU', 'DEXJPUS', 'DEXUSUK',       # Spot exchange rates\n",
    "    'CBBTCUSD', 'CBETHUSD',                # Cryptocurrencies\n",
    "    'T10YIE', 'DCOILBRENTEU',              # Breakeven inflation, Brent oil price\n",
    "    'VIXCLS', 'CBSICO',                    # Implied volatilities, consumer sentiment\n",
    "    'DAAA', 'DBAA',                        # Corporate bond yields\n",
    "    'AMERIBOR', 'T5YIE', 'BAMLH0A0HYM2', 'BAMLH0A0HYM2EY', \n",
    "    'DGS1', 'DCOILWTICO', 'DHHNGSP',        # Additional economic indicators\n",
    "    'ALTSALES',                             # Light Weight Vehicle Sales: Autos and Light Trucks\n",
    "    \n",
    "]\n",
    "\n",
    "# also look at https://tradingeconomics.com/commodity/\n",
    "\n",
    "# SP500 = fred.get_series('SP500')\n",
    "# SP500.name = 'SP500'\n",
    "# df_fred = SP500\n",
    "\n",
    "# # merge data series\n",
    "# for i in range(0, len(varList)):\n",
    "#     series_info = fred.get_series_info(varList[i])\n",
    "\n",
    "#     # Extract the title\n",
    "#     title = series_info['title']\n",
    "#     print(varList[i],'-', title)\n",
    "    \n",
    "#     data = fred.get_series(varList[i])\n",
    "#     data.name = varList[i]\n",
    "#     df_fred = pd.merge(df_fred, data, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4cb9f-a5aa-4051-9167-98aee2c7bc24",
   "metadata": {},
   "source": [
    "####  Please use the features/factors you take and discovered (e.g. FRED, Fama-French website, ADS, momentum factors, technical indicators, volume, price/return lags, etc.) to construct a feature database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8617325-24d6-4a47-b26a-e3e9de5f848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fredapi import Fred\n",
    "import yfinance as yf\n",
    "\n",
    "# 1. FAMA-FRENCH 5 FACTORS\n",
    "# Get the Fama French 5 factor model (monthly data)\n",
    "data_ff5 = pd.read_csv('FF_Research_Data_5_Factors_daily.csv')\n",
    "data_ff5['date'] = data_ff5['date'].astype(str).str[0:4]+'-'\\\n",
    "                    +data_ff5['date'].astype(str).str[4:6]+'-'\\\n",
    "                    +data_ff5['date'].astype(str).str[6:8]\n",
    "                    \n",
    "data_ff5['date'] = pd.to_datetime(data_ff5['date'])\n",
    "# data_ff5['date'] = data_ff5['date'].dt.date\n",
    "df_ffs = data_ff5.set_index('date')\n",
    "\n",
    "# 2. ADS INDEX\n",
    "data_ads = pd.read_excel('ADS_Index_Most_Current_Vintage.xlsx')\n",
    "data_ads['date'] = pd.to_datetime(data_ads['date'])\n",
    "df_ads = data_ads.set_index('date')\n",
    "\n",
    "\n",
    "# 3. FRED DATA\n",
    "fred = Fred(api_key='e2b9d6351c036c7e0a59b80b5713645a')\n",
    "varList = ['T10Y3M', 'DGS10', 'OBMMIJUMBO30YF',  # term premium 10yr-3mon, 30 yr mortgage jumbo loan\n",
    "           'DEXUSEU', 'DEXJPUS', 'DEXUSUK', # spot exchange rates to EUR, JPY, GBP \n",
    "           'CBBTCUSD', 'CBETHUSD',  # cryptocurrencies\n",
    "               'T10YIE', 'DCOILBRENTEU', # breakeven inflation + brent oil price \n",
    "               'VIXCLS', # implied volatilities\n",
    "               'DAAA', 'DBAA', # corporate bond yield\n",
    "              'AMERIBOR', 'T5YIE', 'BAMLH0A0HYM2','BAMLH0A0HYM2EY', 'DGS1', 'DCOILWTICO', \n",
    "                              'DHHNGSP'] \n",
    "SP500 = fred.get_series('SP500')\n",
    "SP500.name = 'SP500'\n",
    "df_fred = SP500\n",
    "# merge data series\n",
    "for i in range(0, len(varList)):\n",
    "    data = fred.get_series(varList[i])\n",
    "    data.name = varList[i]\n",
    "    df_fred = pd.merge(df_fred, data, left_index=True, right_index=True)\n",
    "\n",
    "# Resample Fama-French, ADS, and FRED data to daily frequency\n",
    "df_ffs_daily = df_ffs.resample('D').interpolate(method='linear')\n",
    "df_ads_daily = df_ads.resample('D').interpolate(method='linear')\n",
    "df_fred_daily = df_fred.resample('D').interpolate(method='linear')\n",
    "\n",
    "# 4. STOCK DATA (TESLA)\n",
    "stock_symbol = 'TSLA'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-08-31'\n",
    "stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Add technical indicators\n",
    "\n",
    "# Calculate Simple Moving Averages (SMA)\n",
    "stock_data['SMA_20'] = stock_data['Close'].rolling(window=20).mean()\n",
    "stock_data['SMA_50'] = stock_data['Close'].rolling(window=50).mean()\n",
    "\n",
    "# Calculate Exponential Moving Averages (EMA)\n",
    "stock_data['EMA_20'] = stock_data['Close'].ewm(span=20, adjust=False).mean()\n",
    "stock_data['EMA_50'] = stock_data['Close'].ewm(span=50, adjust=False).mean()\n",
    "\n",
    "def compute_RSI(data, window=14):\n",
    "    diff = data.diff(1)\n",
    "    gain = diff.where(diff > 0, 0).rolling(window=window).mean()\n",
    "    loss = -diff.where(diff < 0, 0).rolling(window=window).mean()\n",
    "    RS = gain / loss\n",
    "    return 100 - (100 / (1 + RS))\n",
    "# Calculate RSI\n",
    "stock_data['RSI'] = compute_RSI(stock_data['Close'], window=14)\n",
    "\n",
    "# Calculate Momentum for multiple periods for Tesla stock\n",
    "momentum_periods = [10, 20, 50]\n",
    "for period in momentum_periods:\n",
    "    stock_data[f'Momentum_{period}'] = stock_data['Close'] - stock_data['Close'].shift(period)\n",
    "\n",
    "def compute_MACD(data, span_long=26, span_short=12, span_signal=9):\n",
    "    EMA_short = data.ewm(span=span_short, adjust=False).mean()\n",
    "    EMA_long = data.ewm(span=span_long, adjust=False).mean()\n",
    "    MACD_line = EMA_short - EMA_long\n",
    "    signal_line = MACD_line.ewm(span=span_signal, adjust=False).mean()\n",
    "    return MACD_line, signal_line\n",
    "\n",
    "stock_data['MACD'], stock_data['Signal_Line'] = compute_MACD(stock_data['Close'])\n",
    "\n",
    "print(stock_data[['SMA_20', 'SMA_50', 'EMA_20', 'EMA_50', 'RSI', 'MACD', 'Signal_Line']].tail())\n",
    "\n",
    "# Moving average of volume\n",
    "stock_data['Volume_MA10'] = stock_data['Volume'].rolling(window=10).mean()\n",
    "\n",
    "# Add lag features\n",
    "# Price lags\n",
    "stock_data['Lag_1'] = stock_data['Close'].shift(1)\n",
    "stock_data['Lag_5'] = stock_data['Close'].shift(5)\n",
    "\n",
    "# Return lags (daily returns)\n",
    "stock_data['Return'] = stock_data['Close'].pct_change()\n",
    "stock_data['Return_Lag1'] = stock_data['Return'].shift(1)\n",
    "stock_data['Return_Lag5'] = stock_data['Return'].shift(5)\n",
    "\n",
    "# Add Bollinger Bands\n",
    "def bollinger_bands(data, window=20, num_of_std=2):\n",
    "    rolling_mean = data['Close'].rolling(window=window).mean()\n",
    "    rolling_std = data['Close'].rolling(window=window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * num_of_std)\n",
    "    lower_band = rolling_mean - (rolling_std * num_of_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "stock_data['Upper_Band'], stock_data['Lower_Band'] = bollinger_bands(stock_data)\n",
    "\n",
    "# Add ATR\n",
    "def atr(data, window=14):\n",
    "    high_low = data['High'] - data['Low']\n",
    "    high_close = np.abs(data['High'] - data['Close'].shift())\n",
    "    low_close = np.abs(data['Low'] - data['Close'].shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "    return true_range.rolling(window=window).mean()\n",
    "\n",
    "stock_data['ATR'] = atr(stock_data)\n",
    "\n",
    "# Add macroeconomic indicators to stock_data\n",
    "gdp_growth = fred.get_series('GDP', observation_start='2020-01-31')\n",
    "unemployment = fred.get_series('UNRATE', observation_start='2020-01-31')\n",
    "stock_data['GDP_Growth'] = gdp_growth.reindex(stock_data.index, method='ffill')\n",
    "stock_data['Unemployment_Rate'] = unemployment.reindex(stock_data.index, method='ffill')\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "DATA = pd.concat([stock_data, df_ffs_daily, df_ads_daily, df_fred_daily], axis=1)\n",
    "print(DATA.columns)\n",
    "DATA['Target'] = DATA['Close'].shift(-1)\n",
    "DATA = DATA.dropna()\n",
    "\n",
    "# Read lithium index\n",
    "lithium_index_data = pd.read_csv('lithium_index_data.csv')\n",
    "\n",
    "# Convert 'Date' to datetime if not already in that format\n",
    "lithium_index_data['Date'] = pd.to_datetime(lithium_index_data['Date'])\n",
    "\n",
    "# Set the 'Date' as the index\n",
    "lithium_index_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Align 'Adj Close' to match the stock_data index and add it as a new column\n",
    "DATA['Lithium_Adj_Close'] = lithium_index_data['Adj Close'].reindex(DATA.index, method='ffill')\n",
    "\n",
    "\n",
    "# Add Elon Musk's sentiment analysis score for fun haha\n",
    "sentiment_analysis_df = pd.read_csv('sentiment_scores.csv')\n",
    "\n",
    "# Convert 'Date' to datetime if not already in that format\n",
    "sentiment_analysis_df['Date'] = pd.to_datetime(sentiment_analysis_df['Date'])\n",
    "\n",
    "# Set the 'Date' as the index\n",
    "sentiment_analysis_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Add the compound score to DATA as column\n",
    "DATA['Elon_Tweet_Sentiment'] = sentiment_analysis_df['compound'].reindex(DATA.index, fill_value=0)\n",
    "\n",
    "# Save to a single CSV file\n",
    "DATA.to_csv('Final_Feature_Database.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b4b96-54e7-4712-88d9-bf11f0530a9c",
   "metadata": {},
   "source": [
    "### Demonstrate the feature selection process if you use regression-based approach (Ridge regression, LASSO, Elastic Net or LARS) . Virtualize the feature importance if you use decision-tree-based approach (random forest, XGBoost). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ff955-623b-4fb4-a20d-4e7fa1aac95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "data =  DATA\n",
    "y = data['Target']\n",
    "X = data.drop(columns=['Target'])\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Regression-based Feature Selection\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_importance = np.abs(ridge.coef_)\n",
    "\n",
    "# LASSO Regression\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_importance = np.abs(lasso.coef_)\n",
    "\n",
    "# ElasticNet Regression\n",
    "elastic_net = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "elastic_net_importance = np.abs(elastic_net.coef_)\n",
    "\n",
    "# Compile importance into a DataFrame for comparison\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Ridge': ridge_importance,\n",
    "    'LASSO': lasso_importance,\n",
    "    'ElasticNet': elastic_net_importance\n",
    "}).set_index('Feature')\n",
    "\n",
    "# Sort by Ridge importance for better visualization\n",
    "importance_df.sort_values('Ridge', ascending=False, inplace=True)\n",
    "\n",
    "# 2. Tree-based Feature Selection\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_importance = rf.feature_importances_\n",
    "\n",
    "# Add Random Forest importance to the DataFrame\n",
    "importance_df['RandomForest'] = rf_importance\n",
    "\n",
    "# Ridge Regression Feature Importance Visualization\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.barh(X.columns, ridge_importance, color='blue')\n",
    "plt.title('Feature Importance - Ridge Regression')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LASSO Regression Feature Importance Visualization\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.barh(X.columns, lasso_importance, color='green')\n",
    "plt.title('Feature Importance - LASSO Regression')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ElasticNet Feature Importance Visualization\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.barh(X.columns, elastic_net_importance, color='purple')\n",
    "plt.title('Feature Importance - ElasticNet Regression')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Random Forest Feature Importance Visualization\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.barh(X.columns, rf_importance, color='orange')\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "importance_df[['Ridge', 'LASSO', 'ElasticNet', 'RandomForest']].head(10).plot(kind='bar', figsize=(14, 7))\n",
    "plt.title('Top 10 Feature Importances Across Methods')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Method')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea0640-61db-438a-b8ff-5567705153b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Helper function to evaluate models\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, name):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    print(f\"{name} Performance:\")\n",
    "    print(f\"  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"  Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    return test_r2\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_r2 = evaluate_model(ridge, X_train, y_train, X_test, y_test, \"Ridge Regression\")\n",
    "\n",
    "# LASSO Regression\n",
    "lasso_r2 = evaluate_model(lasso, X_train, y_train, X_test, y_test, \"LASSO Regression\")\n",
    "\n",
    "# ElasticNet Regression\n",
    "elastic_net_r2 = evaluate_model(elastic_net, X_train, y_train, X_test, y_test, \"ElasticNet Regression\")\n",
    "\n",
    "# Random Forest\n",
    "rf_r2 = evaluate_model(rf, X_train, y_train, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Compare the R² values\n",
    "results = {\n",
    "    \"Ridge Regression\": ridge_r2,\n",
    "    \"LASSO Regression\": lasso_r2,\n",
    "    \"ElasticNet Regression\": elastic_net_r2,\n",
    "    \"Random Forest\": rf_r2\n",
    "}\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"The most accurate model is: {best_model} with R² = {results[best_model]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758eabc",
   "metadata": {},
   "source": [
    "### Model Training and Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26200579-da47-4a76-8d52-1272cbcde3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - authored by Sahana\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(columns=['Target'])  # All features except the target\n",
    "y = data['Target']  # Next day's stock price (shifted Close price)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the data shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Random Forest model trained successfully.\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Predict the next day based on the last available data\n",
    "latest_data = X.iloc[-1:]  # Extract the most recent feature data\n",
    "next_day_prediction = rf_model.predict(latest_data)[0]\n",
    "\n",
    "print(f\"Predicted Stock Price for the Next Day: ${next_day_prediction:.2f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot actual vs predicted stock prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.values, label='Actual Prices', color='blue')\n",
    "plt.plot(y_pred, label='Predicted Prices', color='orange', linestyle='--')\n",
    "plt.title('Actual vs Predicted Stock Prices')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ff6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - authored by JiachenLu(SVR)\n",
    "# Import necessary libraries\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data (assuming `df` contains the stock data with 'Close' column for closing prices)\n",
    "stock = stock.dropna()  # Ensure no missing values\n",
    "prices = stock['Close'].values\n",
    "dates = np.arange(len(prices)).reshape(-1, 1)  # Use indices as features\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "dates_scaled = scaler.fit_transform(dates)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dates_scaled, prices, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Train the SVR model\n",
    "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates[-len(y_test):], y_test, label=\"Actual Prices\", color='blue')\n",
    "plt.plot(dates[-len(y_test):], y_pred, label=\"Predicted Prices\", color='orange', linestyle='--')\n",
    "plt.title(\"Support Vector Regression - Stock Price Prediction\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 - authored by ZenanFan\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('./Final_Feature_Database.csv')  \n",
    "\n",
    "# Select features (all except non-numeric and the target variable)\n",
    "features = data.select_dtypes(include=['float64', 'int64']).drop(columns=['Target']).columns\n",
    "X = data[features]\n",
    "y = data['Target']\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM model\n",
    "svm_model = SVR(kernel='rbf')  # Using RBF kernel for flexibility\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate performance on both training and testing sets\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE for both training and testing sets\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_rmse, test_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc76813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model by songa pradeepthi\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "# Example DataFrame setup (replace this with your dataset)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Replace this mock data with your actual dataset\n",
    "data = {\n",
    "   \"feature1\": np.random.rand(100),\n",
    "   \"feature2\": np.random.rand(100),\n",
    "   \"feature3\": np.random.rand(100),\n",
    "   \"target_column\": np.random.rand(100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(\"target_column\", axis=1)\n",
    "y = df[\"target_column\"]\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Initialize and train Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "# Predict on test data\n",
    "y_pred = linear_model.predict(X_test)\n",
    "# Evaluate the model using root_mean_squared_error\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "# Coefficients of the Linear Regression model\n",
    "print(\"Model Coefficients:\", linear_model.coef_)\n",
    "print(\"Model Intercept:\", linear_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb2e94-df65-4ada-a468-e9b7ee37fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 - authored by Zuyi\n",
    "# Gradient Boosting Regressor Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "data = pd.read_csv('Final_Feature_Database.csv', index_col=0, parse_dates=True)\n",
    "target_column = 'Target'\n",
    "\n",
    "# Ensure the target variable exists\n",
    "if target_column not in data.columns:\n",
    "    raise ValueError(f\"{target_column} not found in the dataset.\")\n",
    "\n",
    "# Select all features\n",
    "features = [col for col in data.columns if col != target_column]\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X = scaled_data[features]\n",
    "y = scaled_data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Train Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=9,\n",
    "    min_samples_split=18,\n",
    "    min_samples_leaf=5,\n",
    "    subsample=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Testing RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Step 4: Visualize results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='Actual', color='blue')\n",
    "plt.plot(y_test_pred, label='Predicted', color='orange', linestyle='--')\n",
    "plt.title('Gradient Boosting Regressor: Actual vs Predicted')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Price/Return')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1470b3-6eb0-4f87-98c2-bfffbf34d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6 - Authored by Yinhao Zhao\n",
    "# XGBoost Regressor Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "data = pd.read_csv('Final_Feature_Database.csv', index_col=0, parse_dates=True)\n",
    "target_column = 'Target'\n",
    "\n",
    "# Ensure the target variable exists\n",
    "if target_column not in data.columns:\n",
    "    raise ValueError(f\"'{target_column}' not found in the dataset.\")\n",
    "\n",
    "# Select all features except the target\n",
    "features = [col for col in data.columns if col != target_column]\n",
    "data = data.dropna()\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data[features]\n",
    "y = data[target_column]\n",
    "\n",
    "# Step 2: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=features, index=X.index)\n",
    "\n",
    "# Step 3: Split into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "\n",
    "# Step 4: Initialize XGBoost Regressor with Parameters\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 5: Train the XGBoost Regressor\n",
    "print(\"\\nTraining XGBoost Regressor...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the Model\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"\\nTraining RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Testing RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Step 8: Visualize Actual vs Predicted Prices\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='Actual Prices', color='blue')\n",
    "plt.plot(y_test_pred, label='Predicted Prices', color='orange', linestyle='--')\n",
    "plt.title('XGBoost Regressor: Actual vs Predicted Stock Prices')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Price/Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Plot Feature Importances\n",
    "importances = xgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the top 20 features\n",
    "print(\"\\nTop 20 Feature Importances:\")\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "# Plot the top 20 feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance_df['Feature'][:20], feature_importance_df['Importance'][:20], color='skyblue')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.title('Top 20 Feature Importances (XGBoost Regressor)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb46b7-728b-4cd4-b130-9c655c3efa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training and Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmark Study(Assignment2 Garch & Kalman)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764b30b",
   "metadata": {},
   "source": [
    "## Garch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764476c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GARCH likelihood function\n",
    "def garch(param, Y, T):\n",
    "    # Initialize Params\n",
    "    mu = param[0]\n",
    "    omega = param[1]\n",
    "    alpha = param[2]\n",
    "    beta = param[3]\n",
    "    \n",
    "    # Initialize values\n",
    "    sigma2 = np.zeros(T)\n",
    "    sigma2[0] = np.var(Y)\n",
    "    Likelihood = 0\n",
    "    \n",
    "    # Calculate the likelihood\n",
    "    for t in range(1, T):\n",
    "        sigma2[t] = omega + alpha * ((Y[t-1] - mu) ** 2) + beta * sigma2[t-1]\n",
    "        F_t = Y[t] - mu\n",
    "        v_t = sigma2[t]\n",
    "        GARCH_Dens_t = (1 / 2) * np.log(2 * np.pi) + (1 / 2) * np.log(v_t) + (1 / 2) * (F_t ** 2) / v_t\n",
    "        Likelihood += GARCH_Dens_t  \n",
    "    \n",
    "    return Likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff94bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to generate the GARCH path\n",
    "def garch_path(params, Y, T):\n",
    "    mu = params[0]\n",
    "    omega = params[1]\n",
    "    alpha = params[2]\n",
    "    beta = params[3]\n",
    "    \n",
    "    sigma2 = np.zeros(T)\n",
    "    path = np.zeros(T)\n",
    "    \n",
    "    # Initialize volatility and path\n",
    "    sigma2[0] = np.var(Y)\n",
    "    path[0] = Y[0]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        sigma2[t] = omega + alpha * ((path[t-1] - mu) ** 2) + beta * sigma2[t-1]\n",
    "        path[t] = mu + np.sqrt(sigma2[t]) * np.random.normal(0, 1)\n",
    "        \n",
    "    return path, sigma2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84486812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load stock data (assuming 'stock' DataFrame exists)\n",
    "Y = np.diff(np.log(stock['Adj Close'].values))\n",
    "T = Y.shape[0]\n",
    "\n",
    "# Initial parameter guess\n",
    "param0 = np.array([np.mean(Y), np.var(Y) / 400, 0.1, 0.03])\n",
    "\n",
    "# Minimize the negative log likelihood\n",
    "results = minimize(garch, param0, args=(Y, T), method='BFGS', tol=1e-2, options={'disp': True})\n",
    "param_star = results.x\n",
    "\n",
    "# Generate GARCH path using the optimized parameters\n",
    "path, vol = garch_path(param_star, Y, T)\n",
    "Y_GARCH = path\n",
    "\n",
    "# Plotting Actual vs Predicted Log Returns\n",
    "timevec = np.linspace(1, T, T)\n",
    "plt.plot(timevec, Y, 'b', label='Actual Returns')\n",
    "plt.plot(timevec, Y_GARCH, 'r', label='GARCH Predicted Returns')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Log Returns')\n",
    "plt.title('GARCH Model Prediction vs Actual Log Returns (Tesla)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE = np.sqrt(np.mean((Y_GARCH - Y) ** 2))\n",
    "print(f'RMSE value is: {RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb6607",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kalman filter code to predict Tesla's daily stock prices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from pandas_datareader import DataReader\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "stock_symbol = 'TSLA'\n",
    "\n",
    "# Download stock data\n",
    "stock = yf.download(stock_symbol, start_date, end_date)\n",
    "def kalman_filter(param, *args):\n",
    "    # initialize params\n",
    "    Z = param[0]\n",
    "    T = param[1]\n",
    "    H = param[2]\n",
    "    Q = param[3]\n",
    "    # initialize vector values:\n",
    "    u_predict, u_update, P_predict, P_update, v, F = {}, {}, {}, {}, {}, {}\n",
    "    Y, S = args\n",
    "    u_update[0] = Y[0]\n",
    "    u_predict[0] = u_update[0]\n",
    "    P_update[0] = np.var(Y) / 4\n",
    "    P_predict[0] = T * P_update[0] * np.transpose(T) + Q\n",
    "    Likelihood = 0\n",
    "    for s in range(1, S):\n",
    "        F[s] = Z * P_predict[s - 1] * np.transpose(Z) + H\n",
    "        v[s] = Y[s - 1] - Z * u_predict[s - 1]\n",
    "        u_update[s] = u_predict[s - 1] + P_predict[s - 1] * np.transpose(Z) * (1 / F[s]) * v[s]\n",
    "        u_predict[s] = T * u_update[s]\n",
    "        P_update[s] = P_predict[s - 1] - P_predict[s - 1] * np.transpose(Z) * (1 / F[s]) * Z * P_predict[s - 1]\n",
    "        P_predict[s] = T * P_update[s] * np.transpose(T) + Q\n",
    "        Likelihood += (1 / 2) * np.log(2 * np.pi) + (1 / 2) * np.log(abs(F[s])) + (1 / 2) * np.transpose(v[s]) * (1 / F[s]) * v[s]\n",
    "\n",
    "    return Likelihood\n",
    "\n",
    "def kalman_smoother(params, *args):\n",
    "    # initialize params\n",
    "    Z = params[0]\n",
    "    T = params[1]\n",
    "    H = params[2]\n",
    "    Q = params[3]\n",
    "    # initialize vector values:\n",
    "    u_predict, u_update, P_predict, P_update, v, F = {}, {}, {}, {}, {}, {}\n",
    "    Y, S = args\n",
    "    u_update[0] = Y[0]\n",
    "    u_predict[0] = u_update[0]\n",
    "    P_update[0] = np.var(Y) / 4\n",
    "    P_predict[0] = T * P_update[0] * np.transpose(T) + Q\n",
    "    for s in range(1, S):\n",
    "        F[s] = Z * P_predict[s - 1] * np.transpose(Z) + H\n",
    "        v[s] = Y[s - 1] - Z * u_predict[s - 1]\n",
    "        u_update[s] = u_predict[s - 1] + P_predict[s - 1] * np.transpose(Z) * (1 / F[s]) * v[s]\n",
    "        u_predict[s] = T * u_update[s]\n",
    "        P_update[s] = P_predict[s - 1] - P_predict[s - 1] * np.transpose(Z) * (1 / F[s]) * Z * P_predict[s - 1]\n",
    "        P_predict[s] = T * P_update[s] * np.transpose(T) + Q\n",
    "\n",
    "    u_smooth, P_smooth = {}, {}\n",
    "    u_smooth[S - 1] = u_update[S - 1]\n",
    "    P_smooth[S - 1] = P_update[S - 1]\n",
    "    for t in range(S - 1, 0, -1):\n",
    "        u_smooth[t - 1] = u_update[t] + P_update[t] * np.transpose(T) / P_predict[t] * (u_smooth[t] - T * u_update[t])\n",
    "        P_smooth[t - 1] = P_update[t] + P_update[t] * np.transpose(T) / P_predict[t] * (P_smooth[t] - P_predict[t]) / P_predict[t] * T * P_update[t]\n",
    "\n",
    "    smooth_path = u_smooth\n",
    "    return smooth_path\n",
    "\n",
    "Y = stock['Adj Close'].values\n",
    "S = Y.shape[0]\n",
    "\n",
    "# Initial parameters\n",
    "param0 = np.array([0.24, 4.25, np.var(Y) / 2000, np.var(Y) / 50])\n",
    "results = minimize(kalman_filter, param0, args=(Y, S), method='BFGS', tol=1e-2, options={'disp': True})\n",
    "\n",
    "# Smoothe and visualize the estimated path\n",
    "param_star = results.x\n",
    "path = kalman_smoother(param_star, Y, S)\n",
    "sorted_path = dict(sorted(path.items()))\n",
    "Y_kalmanFilter = sorted_path.values()\n",
    "\n",
    "# Plotting results\n",
    "timevec = np.linspace(1, S, S)\n",
    "plt.title('Kalman Filter Stock Price Prediction: ' + stock_symbol)\n",
    "plt.plot(timevec, Y_kalmanFilter, 'r', label='Kalman Filter Prediction')\n",
    "plt.plot(timevec, Y, 'b:', label='Actual Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Extracting the dictionary values into a list (in the same order as keys)\n",
    "kalman_values = list(Y_kalmanFilter)\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE = np.sqrt(np.mean((np.array(kalman_values) - np.array(Y)) ** 2))\n",
    "print('RMSE value is: $', RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Rules and Signal Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9acf95",
   "metadata": {},
   "source": [
    "## Trading Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60ea06",
   "metadata": {},
   "source": [
    "### Rule 1: SMA Crossing Strategy\n",
    "#### Logic: \n",
    "buy when a short-term SMA (e.g. SMA_20) crosses a long-term SMA (e.g. SMA_50); sell when a short-term SMA crosses a long-term SMA.\n",
    "#### Condition:\n",
    "Buy signal: SMA_20 > SMA_50 and the previous day SMA_20 <= SMA_50\n",
    "Sell signal: SMA_20 < SMA_50 and previous day SMA_20 >= SMA_50\n",
    "### Rule 2: Momentum Strategy\n",
    "#### Logic: \n",
    "buy when momentum (Momentum_20) is positive and increasing; sell when it is negative and decreasing.\n",
    "#### Condition:\n",
    "Buy signal: Momentum_20 > 0 and Momentum_20 > previous day's Momentum_20\n",
    "Sell signal: Momentum_20 < 0 and Momentum_20 < Momentum_20 of the previous day.\n",
    "### Rule 3: Bollinger Band Strategy\n",
    "#### Logic: \n",
    "Sell when price breaks above the upper Bollinger Band; Buy when it falls below the lower band.\n",
    "#### Conditions:\n",
    "Buy signal: Close < Lower_Band\n",
    "Sell signal: Close > Upper_Band\n",
    "### Rule 4: Sentiment-based strategies\n",
    "#### Logic: \n",
    "utilize the Elon_Tweet_Sentiment indicator. Buy when sentiment is significantly positive; sell when significantly negative.\n",
    "#### Condition:\n",
    "Buy signal: Elon_Tweet_Sentiment > some threshold (e.g. 0.2)\n",
    "Sell signal: Elon_Tweet_Sentiment < some threshold (e.g. -0.2）\n",
    "\n",
    "Translated with DeepL.com (free version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa63b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1 means buy, -1 means sell, 0 means stay the same\n",
    "\n",
    "# # Rule 1: Moving Average Crossover\n",
    "# def moving_average_crossover(df):\n",
    "#     df['Signal_MA'] = 0\n",
    "#     df['Signal_MA'] = np.where((df['SMA_20'] > df['SMA_50']) & (df['SMA_20'].shift(1) <= df['SMA_50'].shift(1)), 1, df['Signal_MA'])\n",
    "#     df['Signal_MA'] = np.where((df['SMA_20'] < df['SMA_50']) & (df['SMA_20'].shift(1) >= df['SMA_50'].shift(1)), -1, df['Signal_MA'])\n",
    "#     return df\n",
    "\n",
    "# # Rule 2: Momentum Strategy\n",
    "# def momentum_strategy(df):\n",
    "#     df['Signal_Momentum'] = 0\n",
    "#     df['Signal_Momentum'] = np.where((df['Momentum_20'] > 0) & (df['Momentum_20'] > df['Momentum_20'].shift(1)), 1, df['Signal_Momentum'])\n",
    "#     df['Signal_Momentum'] = np.where((df['Momentum_20'] < 0) & (df['Momentum_20'] < df['Momentum_20'].shift(1)), -1, df['Signal_Momentum'])\n",
    "#     return df\n",
    "\n",
    "# # Rule 3: Bollinger Bands Strategy\n",
    "# def bollinger_band_strategy(df):\n",
    "#     df['Signal_Bollinger'] = 0\n",
    "#     df['Signal_Bollinger'] = np.where(df['Close'] < df['Lower_Band'], 1, df['Signal_Bollinger'])\n",
    "#     df['Signal_Bollinger'] = np.where(df['Close'] > df['Upper_Band'], -1, df['Signal_Bollinger'])\n",
    "#     return df\n",
    "\n",
    "# # Rule 4: Sentiment Strategy\n",
    "# def sentiment_based_strategy(df, sentiment_threshold=0.2):\n",
    "#     df['Signal_Sentiment'] = 0\n",
    "#     df['Signal_Sentiment'] = np.where(df['Elon_Tweet_Sentiment'] > sentiment_threshold, 1, df['Signal_Sentiment'])\n",
    "#     df['Signal_Sentiment'] = np.where(df['Elon_Tweet_Sentiment'] < -sentiment_threshold, -1, df['Signal_Sentiment'])\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Final_Feature_Database.csv'  # 文件路径\n",
    "feature_database = pd.read_csv(file_path)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Extract relevant features and the target variable from the dataset\n",
    "features = feature_database.select_dtypes(include=['float64', 'int64']).drop(columns=['Target']).columns\n",
    "X = feature_database[features]\n",
    "y = feature_database['Target']\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM model\n",
    "svm_model = SVR(kernel='rbf')  # Using RBF kernel for flexibility\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the entire dataset to simulate trading\n",
    "predicted_prices = svm_model.predict(X)\n",
    "\n",
    "# Add predictions to the dataset\n",
    "feature_database['Predicted_Target'] = predicted_prices\n",
    "\n",
    "# Define trading signals\n",
    "# Buy signal: predicted price > current close price\n",
    "# Sell signal: predicted price < current close price\n",
    "feature_database['Signal'] = feature_database.apply(\n",
    "    lambda row: 'Buy' if row['Predicted_Target'] > row['Close'] else 'Sell',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add a column to track daily returns\n",
    "feature_database['Daily_Return'] = feature_database['Close'].pct_change()\n",
    "\n",
    "# Calculate cumulative profits and losses based on signals\n",
    "initial_investment = 100000  # Starting capital\n",
    "feature_database['Profit/Loss'] = feature_database['Signal'].shift().apply(\n",
    "    lambda signal: initial_investment * feature_database['Daily_Return'] if signal == 'Buy' else 0\n",
    ").cumsum()\n",
    "\n",
    "# Display the results for review\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Trading Simulation Results\", dataframe=feature_database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# # 将日期列设为索引，便于绘图\n",
    "# feature_database['Unnamed: 0'] = pd.to_datetime(feature_database['Unnamed: 0'])\n",
    "# feature_database.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "# 绘制股价和交易信号图\n",
    "fig_price_signals = go.Figure()\n",
    "\n",
    "# 添加收盘价折线图\n",
    "fig_price_signals.add_trace(go.Scatter(\n",
    "    x=feature_database.index,\n",
    "    y=feature_database['Close'],\n",
    "    mode='lines',\n",
    "    name='Close Price'\n",
    "))\n",
    "\n",
    "# 添加买入信号\n",
    "buy_signals = feature_database[feature_database['Signal'] == 'Buy']\n",
    "fig_price_signals.add_trace(go.Scatter(\n",
    "    x=buy_signals.index,\n",
    "    y=buy_signals['Close'],\n",
    "    mode='markers',\n",
    "    name='Buy Signal',\n",
    "    marker=dict(symbol='triangle-up', color='green', size=10)\n",
    "))\n",
    "\n",
    "# 添加卖出信号\n",
    "sell_signals = feature_database[feature_database['Signal'] == 'Sell']\n",
    "fig_price_signals.add_trace(go.Scatter(\n",
    "    x=sell_signals.index,\n",
    "    y=sell_signals['Close'],\n",
    "    mode='markers',\n",
    "    name='Sell Signal',\n",
    "    marker=dict(symbol='triangle-down', color='red', size=10)\n",
    "))\n",
    "\n",
    "fig_price_signals.update_layout(\n",
    "    title=\"Stock Prices with Trading Signals\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Stock Price (USD)\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# 绘制每日回报图\n",
    "fig_daily_returns = go.Figure()\n",
    "fig_daily_returns.add_trace(go.Scatter(\n",
    "    x=feature_database.index,\n",
    "    y=feature_database['Daily_Return'],\n",
    "    mode='lines',\n",
    "    name='Daily Return'\n",
    "))\n",
    "fig_daily_returns.update_layout(\n",
    "    title=\"Daily Returns\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Return\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "\n",
    "# 显示交互式图表\n",
    "fig_price_signals.show()\n",
    "fig_daily_returns.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce0c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
